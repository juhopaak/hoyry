{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from libvoikko import Voikko\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voikko = Voikko(\"fi\")\n",
    "\n",
    "def clean(s):\n",
    "    s = re.sub(\"\\n\", \" \", s)\n",
    "    s = re.sub(\"  \", \" \", s)\n",
    "    s = s.strip()\n",
    "    return(s)\n",
    "\n",
    "def remove_punct(s):\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    s = re.sub(\"§(\\S+)?\", \"\", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    s = re.sub(\"(^| ).( |$)\", \" \", s)\n",
    "    s = re.sub(\"–\", \"\", s)\n",
    "    s = re.sub('”', \"\", s)\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    return(s)\n",
    "\n",
    "def tokenize_sentences(text, voikko):\n",
    "    return [s.sentenceText.strip() for s in voikko.sentences(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINLEX-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/kho/\" + x for x in os.listdir(\"data/kho\")]\n",
    "paths += [\"data/kko/\" + x for x in os.listdir(\"data/kko\")]\n",
    "paths += [\"data/sd/\" + x for x in os.listdir(\"data/sd\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for p in paths:\n",
    "    with open(p, 'r') as f:\n",
    "        data += json.load(f)\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"year\", inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "df.drop(\"index\", 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df['text'])\n",
    "\n",
    "sentences = []\n",
    "for i in range(0, len(texts)):\n",
    "    tokens = tokenize_sentences( clean(texts[i]), voikko )\n",
    "    sentences += [remove_punct(t).split() for t in tokens]\n",
    "\n",
    "del texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2277594"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py\n",
    "model_ft = FastText(sentences, size = 200, window = 5, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=211136, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.save(\"models/fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec(sentences, size = 200, window = 5, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=211136, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.save(\"models/w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ennustetaan runoaineistolle lakimalliin pohjaavat uudet sanat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/cutups/\" + x for x in os.listdir(\"data/cutups\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for p in paths:\n",
    "    with open(p, \"r\") as f:\n",
    "        text = f.read()\n",
    "        text = clean(text)\n",
    "        text = remove_punct(text)\n",
    "        \n",
    "        data.append( text.split() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(\"models/w2v\")\n",
    "model_ft = FastText.load(\"models/fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in total: 1085\n",
      "Words inferred with W2V model: 771\n",
      "Words inferred with FT model: 314\n"
     ]
    }
   ],
   "source": [
    "pred = [ [] for i in range(12) ]\n",
    "\n",
    "words = 0\n",
    "w2v_pred = 0\n",
    "ft_pred = 0\n",
    "\n",
    "for runo_pos in range(len(data)):\n",
    "    \n",
    "    runo = data[runo_pos]\n",
    "    \n",
    "    for w_pos in range(len(runo)):\n",
    "        \n",
    "        words += 1\n",
    "        \n",
    "        # Yritetään päätellä uusi sana käyttäen w2v-mallia\n",
    "        try:\n",
    "            pred[runo_pos].append(model_w2v.wv.most_similar(runo[w_pos])[0])\n",
    "            w2v_pred += 1\n",
    "\n",
    "        # Jos sana ei ole sanastossa, käytetään FastText-mallin lähintä sanaa tai alkuperäistä sanaa\n",
    "        except:\n",
    "            #pred[runo_pos].append((runo[w_pos], 1))\n",
    "            pred[runo_pos].append(model_ft.wv.most_similar(runo[w_pos])[0])\n",
    "            ft_pred += 1 \n",
    "        \n",
    "        # Jos malli ei tunne sanaa, käytetään FastText-mallin lähintä sanaa\n",
    "        #if runo[w_pos] not in model_w2v.wv.vocab:\n",
    "            #pred[runo_pos].append(model_ft.wv.most_similar(runo[w_pos])[0])\n",
    "            #ft_pred += 1\n",
    "        \n",
    "        # Muuten käytetään alkuperäistä sanaa\n",
    "        #else:\n",
    "            #pred[runo_pos].append((runo[w_pos], 1))\n",
    "            \n",
    "        \n",
    "\n",
    "print(\"Words in total: \"+ str(words))\n",
    "print(\"Words inferred with W2V model: \" + str(w2v_pred))\n",
    "print(\"Words inferred with FT model: \" + str(ft_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for p in reversed(pred):\n",
    "    \n",
    "    text = \"\"\n",
    "    prob = \"\"\n",
    "    \n",
    "    for word in p:\n",
    "        text += word[0] + \" \"\n",
    "        prob += str(word[1]) + \", \"\n",
    "    \n",
    "    with open(\"models/cutups/\" + str(i) + \"_kaikki.txt\", \"w\") as f:\n",
    "        f.write(text + \" \" + prob)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasketaan sanoilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(\"models/w2v\")\n",
    "model_ft = FastText.load(\"models/fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    'elämä','kuolema','taide','ilmaisu','sana','puhe',\n",
    "    'runo','tyhjä', 'täysi','totuus','todellisuus','kauneus',\n",
    "    'kaunis','yhteisö', 'yhteiskunta','mieli','sielu',\n",
    "    'ruumiillinen','himoita','ääretön','rajaton','ajaton',\n",
    "    'uskomaton','mainio','ovela','vilpitön','aikomus',\n",
    "    'halu','tahto','tarve','tunne','luokkataistelu','toimeentulo',\n",
    "    'laki','oikeus','hallitsija','päättäjä','koti','asunto','asuminen'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laske_sanoilla(sanat):\n",
    "    \n",
    "    text = \"\"\n",
    "    text += \"Sanat: \" + sanat[0] + \", \" + sanat[1] + '\\n\\n'\n",
    "    \n",
    "    model = model_ft\n",
    "    if (sanat[0] in model_w2v.wv.vocab) and (sanat[1] in model_w2v.wv.vocab):\n",
    "        model = model_w2v\n",
    "        \n",
    "    text += \"Malli: \" + str(model) + '\\n\\n'\n",
    "    \n",
    "    for s in sanat:\n",
    "        text += \"Lähin merkitys sanalle '\" + s + \"': \" + model.wv.most_similar(s)[0][0] + '\\n'\n",
    "    text += '\\n'\n",
    "        \n",
    "    vec_add = np.add(model.wv[sanat[0]], model.wv[sanat[1]])\n",
    "    vec_sub = np.subtract(model.wv[sanat[0]],model.wv[sanat[1]])\n",
    "    \n",
    "    text += sanat[0] + \" + \" + sanat[1] + \":\" + '\\n'\n",
    "    pred = model.wv.most_similar(positive=[vec_add], topn=5)\n",
    "    for p in pred:\n",
    "        text += p[0] + \", \" + str(p[1]) + '\\n'\n",
    "    text += '\\n'\n",
    "    \n",
    "    text += sanat[0] + \" - \" + sanat[1] + \":\" + '\\n'\n",
    "    pred = model.wv.most_similar(positive=[vec_sub], topn=5)\n",
    "    for p in pred:\n",
    "        text += p[0] + \", \" + str(p[1]) + '\\n'\n",
    "    text += '\\n'\n",
    "    \n",
    "    with open(\"models/laskutoimituksia/\" + sanat[0] + \",\" + sanat[1] + \".txt\", \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sanat = random.sample(seeds, 2)\n",
    "    laske_sanoilla(sanat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
